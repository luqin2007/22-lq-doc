使用 LangChain 可以方便的创建内部知识库系统，让 AI 可以准确的利用提供的资料作为上下文解决实际问题

大体上，软件加载文档、分析数据的流程大体分为加载、切分、嵌入、检索、查询等过程

> 这里应当使用各种 AI 中的 Embedding 模型
# 加载

`langchain_community.document_loaders` 包下存在对 `PyPDF`，`Docx2txt` 等工具库的包装，可直接将多种不同类型文档加载为 Document 对象

![[../../../_resources/images/Pasted image 20241108012909.png]]

片段中 `documents` 是一个用于存储读取结果的 Document 数组，后面切分时可以直接对整个数组进行切分处理
# 切分

将文档切为多个片段以符合 AI 提问时的长度限制。

文档切分使用 `langchain.text_splitter` 包下的工具即可。`RecursiveCharacterTextSplitter` 可用于按照给定字符切分字段（`split`），并将长度限定在给定大小（`chunk_size`）之内，支持对 `Document[]` 和字符串进行切分

![[../../../_resources/images/Pasted image 20241108014357.png]]
# 嵌入

Embedding，将切分后的文本转换成词嵌入的形式，存储到向量数据库中，默认使用 Qdrant

> [!note] 词嵌入：Word Embedding，将文字转换为一系列数字，通常为向量。向量参考其含义和上下文，语义上相似或相关的词坐标也接近。

> [!note] 向量数据库：专门用于存储和搜索向量数据的数据库

通过 `Embeddings` 对象实现词嵌入过程（`embed_query` 方法），可直接将其传入 `VectorStore` 向量数据库客户端实现类即可。
- `Embeddings` 用于具体词嵌入过程，`langchain` 提供 `OpenAIEmbeddings` 实现，也可以自己实现类继承自 `BaseModel` 和 `Embeddings`
- `VectorStore` 用于存储文档片段和生成的向量，`langchain` 提供多种数据库实现，详见 `langchain_community/vectorstores/__init__.py`

![[../../../_resources/images/Pasted image 20241108025150.png]]

至此，检索前的所有准备活动结束
# 检索

Retrieval，提问时，根据问题与文档片段的相关性，从数据库中找到与问题相关的嵌入片。
## 相关性


## RetrievalQA 链

1. 实例化 `RetrievalQA` 链
# 输出

Output，将问题和嵌入片传递给 AI 语言模型，并返回答案
