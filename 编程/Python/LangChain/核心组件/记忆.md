LLM 每个会话都是无状态的，每次调用需要用记忆机制将往期聊天内容同时提交。通过为 `ConversationChain` 添加 `memory` 参数即可添加。

留给记忆的键通常为 `{history}`

![[../../../../_resources/images/Pasted image 20241112235135.png]]

#  ConversationBufferMemory

缓冲记忆是最简单的记忆机制实现。每次对话时，都将问答内容记录下来用于下次请求的上下文

![[../../../../_resources/images/Pasted image 20241112000705.png]]

通过 `conversation.memory.buffer`  可以查看往期对话历史记录。

问题：当长时间聊天后，上下文迅速增加
- 消耗大量 Token
- 传输更慢
- 可能有输入上限
# ConversationBufferWindowMemory

在 `ConversationBufferMemory` 的基础上，额外需求一个窗口值 `k`，只保留最近 k 组互动数据以压缩请求长度

> [!note] 即使没有保存全部的记忆，LLM 也会记住部分关键词
# ConversationSummaryMemory

`ConversationBufferWindowMemory` 将历史记录进行汇总，将汇总后的数据传递给 history，适合长对话
- 优点：使用对话记录的总结作为上下文，减少 Token 用量
- 缺点：
	- 对短对话优化程度不大
	- 缺失时间戳，不能区分对话时间和先后顺序
	- 需要额外加一个 LLM 调用以提取汇总信息，增加 Token 使用成本
	- 不能限制对话长度
	- 不能区分近期对话和长期对话

汇总功能也通过 AI 实现，因此往往需要另一个解析器
# ConversationSummaryBufferMemory

对话总结缓冲记忆，是一种混合记忆，总结早期互动并尽量保留最近互动的原始内容
1. 当记录的对话长度在 max_token_limit 字以内时，保留原始对话内容
2. 当对话内容超过 max_token_limit 字时，将超过预设长度的内容提交 LLM 进行总结
![[../../../../_resources/images/Pasted image 20241112234937.png]]
优势：总结较早的互动，尽量保留近期互动
缺点：较短的对话中，总结会消耗更多 Token
# 总结

![[../../../../_resources/images/Pasted image 20241112235204.png]] 有些记忆机制（`ConversationSummaryMemory`，`ConversationSummaryBufferMemory`  等）在对话次数较少时会显著浪费一些 Token，但当对话次数增加，对 Token 的节省效果明显

`ConversationBufferWindowMemory` 最节省 Token，但会遗忘旧对话内容，在某些场合是最佳选择